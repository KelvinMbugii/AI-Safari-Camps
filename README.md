# AI-Safari-Camps

## Responsible AI files: When Algorithms Go Rogue
### Case 1: The Biased Hiring Bot
--What's happening:
A company uses an AI system to screen job applicants. It analyzes resumes and automatically filters out candidates who don't meet certain criteria-like career gaps or non-traditional paths.

-- What's Problematic:
The bot unfairly rejects more women, especially those who've taken time off for caregiving.
-- Improvement Idea:
Audit the training data for bias and retrain the model with diverse applicant profiles.

### Case 2: The OverZealous Proctor
-- What's Happening:
Schools use AI-powered proctoring tools during online exams.
These tools flag students for *Cheating* if they move their eyes too much, fidget, or make noise.

-- What's problematic:
Neurodivergent students or those with anxiety are flagged more often just for acting differently.

-- Improvement Idea:
Redesign the system to allow behaviour flexibility and require human review before penalizing students.
